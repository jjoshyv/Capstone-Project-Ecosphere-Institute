# ğŸŒ Capstone Project â€” Ecosphere Institute  
### Automated Data Ingestion and Partitioned Data Lake using Python & PyArrow

---

## ğŸ§  Project Overview
This project implements an **automated ETL (Extract, Transform, Load) data pipeline** that ingests cleaned environmental datasets and stores them efficiently in a **data lake** (Parquet + DuckDB).  
It is part of the Ecosphere Institute Capstone Initiative to build data-driven, scalable, and query-optimized systems for environmental analytics.

---

## ğŸ¯ Project Goals

âœ… Automate ingestion of cleaned datasets into a structured data lake or database  
âœ… Partition data **by time (monthly/yearly)** and **by location (region/country)** for faster queries  
âœ… Design a **common storage schema** and enforce **schema validation**  
âœ… Use **Python scripts** to manage data loading, transformation, and metadata tracking  
âœ… Document and version-control all scripts in a **GitHub repository**

---

## âš™ï¸ Tech Stack

| Category | Technology |
|-----------|-------------|
| **Programming** | Python 3.11 |
| **Libraries** | `pandas`, `pyarrow`, `duckdb`, `uuid`, `argparse`, `logging` |
| **Data Storage** | Parquet files (Data Lake), DuckDB (Local DB) |
| **Version Control** | Git + GitHub |
| **Environment Management** | Conda virtual environment (`modis_env`) |

---

## ğŸ—‚ï¸ Directory Structure

Capstone-Project-Ecosphere-Institute/
â”‚
â”œâ”€â”€ etl/
â”‚ â”œâ”€â”€ py_etl_parquet.py # Core ETL script for Parquet ingestion
â”‚ â”œâ”€â”€ py_etl_parquet_with_metadata.py # Extended ETL with metadata + validation
â”‚ â”œâ”€â”€ spark_etl_parquet.py # (Optional) Spark version for big data
â”‚
â”œâ”€â”€ data_lake/ # Auto-created partitioned Parquet data
â”‚ â”œâ”€â”€ epa_o3_parquet/
â”‚ â”œâ”€â”€ year=2020/month=01/part-.parquet
â”‚ â”œâ”€â”€ year=2021/month=02/part-.parquet
â”‚
â”œâ”€â”€ fIGURES/ # Visualizations & diagrams
â”‚
â”œâ”€â”€ capstone.duckdb # Local analytical database
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ README.md # Project documentation (this file)
â”œâ”€â”€ .gitignore # Ignored data, env, and temp files
â””â”€â”€ cleaned_datasets/ # Input cleaned CSVs (EPA, NASA, etc.)


---

## ğŸ§© Features

- **Automated CSV discovery**  
  Finds cleaned datasets automatically (`Cleaned_*.csv`) unless specified with `--csv`.

- **Schema Validation**  
  Checks for essential columns (`date`, `O3_ug_m3`, `region`, etc.) before ingestion.

- **Data Partitioning**  
  Writes data to Parquet partitioned by `year` and `month` (and optional `region`).

- **Metadata Tracking**  
  Generates metadata in JSON + SQLite for every ingestion run.

- **DuckDB Integration**  
  Optionally loads Parquet data into a local analytical database.

- **Lightweight & Reproducible**  
  Runs fully in Python â€” no Spark or Hadoop required.

---

## ğŸ§° Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/jjoshyv/Capstone-Project-Ecosphere-Institute.git
cd Capstone-Project-Ecosphere-Institute


