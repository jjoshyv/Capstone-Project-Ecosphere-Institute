# ğŸŒ Automated Data Ingestion and Partitioned Data Lake ETL Pipeline

This repository contains a modular and automated **ETL (Extractâ€“Transformâ€“Load)** pipeline built with **Python**, **Pandas**, and **PyArrow**, designed to ingest, transform, and store cleaned environmental datasets (e.g., EPA Ozone Data) into a **partitioned Parquet Data Lake**.  
The project simulates an end-to-end **data engineering workflow** with metadata tracking and optional **DuckDB** integration for fast analytical queries.

---

## ğŸš€ Project Overview

The pipeline automates the ingestion of cleaned CSV datasets, applies transformations, validates schemas, and loads data into a structured **Parquet Data Lake**.  
It also records metadata for every ETL run and supports partitioning by:
- **Time:** Year and Month  
- **Location:** Region, Country, or Site ID (optional)

This structure improves **query performance**, **data organization**, and **scalability** for analytical workloads.

---

## ğŸ§© Features

âœ… **Automated CSV ingestion** â€” discovers cleaned CSVs or accepts a file path  
âœ… **Case-insensitive schema validation** â€” handles column naming variations (`Date`, `date`, `DATE`)  
âœ… **Partitioned Parquet output** â€” organized by year, month, and optional location  
âœ… **Metadata tracking** â€” JSON + SQLite/DuckDB logs for auditability  
âœ… **Auto-create missing columns** â€” optional fallback for missing location columns  
âœ… **Chunked processing** â€” handles large datasets efficiently  
âœ… **Plug-and-play** â€” runs standalone with minimal setup  

---

## ğŸ—ï¸ Project Structure

